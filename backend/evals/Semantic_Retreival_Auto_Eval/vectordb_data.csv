video_id,timestamp,corrected_text
video_001,0-60,"help us mapping x to y, right? And y used to be the real number, right? That's what we call the regression. And x basically had the dimension. When we want to talk about classification, the idea is that the main change that we have is that y is no longer a real number, and it has k dimensions okay a better way saying it why it belongs to y1 to yk now what are those k is that imagine you want to solve a problem uh create a model that predicts or classifies breeds of the breed of a dog you know and all the breeds that you have those are going to be this yk. Now, k, this k are unique, that means that, you know, if you look at your"
video_001,120-180,"try this as multi-class because majority of the algorithms classification algorithm that we have you cannot use more than two more than binary they are not solving it so by default it's always binary what's your solve now um previously okay we in regression we said h theta x and we find that that wasn called theta transpose x right in regression that gives us a prediction value and we call this either we call it let say y hat or we can say that almost y right We wanted to make that prediction But classification that's no longer the same. Because when you have a data point, and you want to say that, okay, which class the stage of point belongs to, you know, that question is no longer a short,"
video_001,180-240,"like a straight answer like this. You cannot, here we just say that this is a, this is information for a house, what is the price? You know, it's straightforward. But in classification, you cannot write the formula in this formula. You have a feature input, you should say, the way that we we formulate is that what is the chance, okay, it's like a probability, what is the probability, this data belongs to a cat, what's this probability, this, you know, it's a dog. So the formula becomes like this, what is the probability of y being k, you know, this data point belongs to class k, it could be anything, even this input. That's what we want to going with classification. Okay. Now, in regression, when we had the data set,"
video_001,240-300,"x1, y1, all the way to xn, yn. And we would have said that the probability of that data set basically was multiplication of probability of basically each individual, right? X1 and Y1, right? In classification, we don't say that also. We are basically, when we have a data set, so let me express something. So when we want to create a classification model, you always say something is important for you, you want to solve. you know when you really do regression price of the house is something that's important you want to be able to predict it correctly in classification if you have a model that is is going to detect specific disease you know all you care about is the disease prediction not the other way around right so when we're dealing with the binary classification we first need to point out to"
video_001,300-360,"to one class one and class zero. OK all binary problems except for support vector machine we will see later they have this formulation one versus zero Class one is called class of interest And class zero is the other class, basically. Class of interest is what you want to solve. What is the goal of developing this model for you is to be able to detect this. not that one. That's not important. So because this is what is important for us, when we want to formulate it, we focus on this, you know, like what does it mean that is that if we have this data set, this data set, each input could have either class zero or class one, right? And when"
video_001,360-420,we want to formulate this we will say what is the probability of y given x okay we are not going to just leave it like that we actually will say what is the probability of y being class one so in the regression we simply multiplied all the individual probabilities right probability of x1 y1 times probability of x1 x2 y2 times times right in here we also do the same idea iid we do multiplication but we are not so you know another way of writing this could be like this what was what's the problem of y given x if we didn't care about the class right what is the probability of this target value given this input but that's not the way we want to solve it what we care about is what is the probability of being class one given some input so all the for all the
video_001,420-480,"problems algorithms in classification when we want to formulate except again for support recognition we will talk about we always formulate with respect to class one okay and you know the reason for that is that if you solve it with respect to class one this is a probability so class zero becomes one minus last one right so but this is what we want to learn now again remember so any pro any time you also have a data set if your data is in this category you need to do some sort of encoding to zero and one. There are two main types of classification First one is called discriminative And the other one for generated There are two main types of classification, discriminated classification, generative"
video_001,480-540,"classification. Now, the most of this we are trying to solve with this equation, what is the probability of y being something even x. The reason I put k rather than 1 because this is the same formula for multilateral classification also, so k for now. In both scenarios, we are trying to solve this equation. Now, let's take a look at here. This is x1 and this is y. And this was a regression problem, right? Okay, your goal was to find that line, correct? That goes through the point. And every time you pass x, it makes a prediction gives you the y value. Now, when you are dealing with classification,"
video_001,540-600,"let's say you have two dimension x1 and x2. This is the sub that you have. And this is class one. These points on the left are class zero. Your goal here is to find a line that separates two classes from each other. Okay. And this line is theta transpose x equal to zero. We will get to that y vessel equation. But that's that's that's what we want to solve a classification problem. You need to find this line to have this line, then you can say that each class the point belongs. This line called decision boundary or decision line. Okay, so. Now, when you're talking about the discriminating classification. Basically, discriminative classification is the type of classification that all it cares"
video_001,60-120,"dataset and you have one million records, and you look at the column y, and you see you have 10 unique category, then you have 10, 10 is k is 10. So it's not, we don't have one per data, basically you have limited number of k. Now, so anytime that you're trying to solve this same problem, but y is no longer a real number and is one of these k categories that becomes a classification problem. Now, if y is more than two, we call it a multi-label classification. If y is two, basically we call it binary for this class majority of the time we are dealing with the binary classification okay uh otherwise we'll i will specify that if it's not binding so anytime i write a classification algorithm it's by default a binary classification if it's multi-class i will"
video_001,600-660,"about is to find a line, to simply just find a line and calculate this value just with respect to the line. You know, if you have a line, you look at and you calculate that value. That value is either positive or negative. If it's positive, that means you're upside of the line. If it's negative, you are under the line."
video_001,1020-1080,"probability of x given this times the prior. This is generated. So you need to have a prior, you need to have a light. Now, think about it. What is a prior? In classification, what's the prior do you think? Training. What's that? Training. Training. No, but what is the prior? Yeah, from the training data, how would you define? Sum of all the class interests, but the number of rows in the data. Correct. So for class K, you look at your data set, how many cats data point I have is cat divided by total. That's prior. So prior in classification is really easy to calculate. calculate it you just look at your data simply calculate all the priorities that you need right"
video_001,1080-1140,"so you never have a problem in calculating this one the likelihood term or the density term is where you have problems because you don know that and this is you know density or likelihood comes from a distribution right So that means that there is some distribution which you not aware of right So that means that some unknown distribution, you need to estimate that. So how would you estimate an unknown distribution? We already talked like maximum likelihood distribution is one technique, right? So you assume, like for example, you collect some data, large amounts of data, right? and then you are trying to solve for it by using MLE, finding the parameters that you are looking. So that's one approach to use some basically density estimation technique to find that density function. The second approach is to make an assumption, like you make a strong assumption"
video_001,1140-1200,"that okay for this problem that I'm solving I know this data comes from a Poisson distribution, I know this data comes from a Gaussian distribution, right? So anytime you have some unknown distribution, those are the two approaches you can take. You either solve it for it using some of these estimation techniques or you make an assumption. When I say make an assumption also, that doesn't mean that you can't just make any assumption. You technically need to prove it mathematically that the assumption is sound, right? So now in this, this equation in your book is written as this form just I'm just this is the same thing I'm just rewriting it so um nor if you saw it in the book divided by in the book these two terms are the same because this is the likelihood this is the prior and you"
video_001,1200-1260,"know what Bayesian theorem it has some that normalization factor and that's the same normalization So if you saw this term is the same. Now, when we are talking about the LDA, LDA is the classification that the generative classification that has two assumptions. The first part is that in order to calculate the density in LDA, we make a strong assumption that the density comes from a multivariate Gaussian. That's the first assumption that we're making. So here, X even Y equal. Hey, this comes from multivariate Gaussian with mean K and variance."
video_001,600-660,"Discrimitive classification, all it cares about is that straight forward, you make a calculation and you have a line. If it's positive, you say it's class one, it's negative, it should be class zero. So any classification that all it cares about is to find a line and with respect to the line, look at the positive and negatives just by calculating this term directly called discriminative classification. Okay? So any classification that calculate this term using a basically naive based theorem"
video_001,660-720,"becomes generative classification. Because it's naive based, that means this term is the posterior and this equal a likelihood times a prior. Right? So what's the likelihood is the reverse basically of x, u and y equal to x, and this is the Any classification algorithm that solve this problem by calculating this term becomes generative classification. Now, in both scenarios, in any classification, the whole idea is to find that line in any classification, it doesn't matter, neural network or support vector or whatever. There's a line you need to find, that line will separate the points. In discriminative, you have the line, then you calculate this term. This term is a straight, it's just one shot."
video_001,720-780,"And if it's either positive, you are above, if it's negative, you are below. In generative, in order to calculate this equation, you first calculate that, then you multiply it by the prior, then you see the value is either positive or negative. Then you decide it's above or you are below the line. That's the main difference these two have. Let's say we have this dataset and we are interested to see that what is the probability of y1, y2 through yn given x1 through xn for probability of the data. interested to find that. And because we said this is IAD."
video_001,780-840,"What the problem of yi u n x i And we always say that anytime we are solving this problem this is we just went to class of interest Okay this is the technique means this is equal to probability of y i in one u n x i. So you have a data set. This data set is binary. It's either class one or class zero. And in order to find what is the probability of this total, this data set, what's the chance of this? You need to do this multiplication. But if you think about it, there is a problem with this equation. Okay, so we said here that respect to this basic figure. If theta transpose X i is greater than zero, you return plus one."
video_001,840-900,"If theta transpose X i is less than zero, you return plus zero. Great. That's how we need to solve. It's either positive or negative. If it's zero, that's the line. That's why we don't have this. So you're interested to calculate this term with respect to class one. That means for each point, you have endpoints, for each point, you look, what is the probability this point belongs to class one? So how do you calculate that? You look at this term, right? Is there either positive or negative? So that means that for some of the points, this is one, but for some point, this goes to zero, right? So you see what's the problem here? one one times times magic n minus one of them is one one of them is zero this multiplication becomes zero so because we care we solve this with respect to class zero class one so this multiplication"
video_001,900-960,"then that means that every time every data points we have should be class one exactly in order to this code becomes one, otherwise becomes zero. Okay, so that's the first problem that general form of classification has and we need to basically address. Now we will come back to this very quickly, keep this in mind. Now, again, so this is the overall way of of formulating a classification this is what we just wrote here You have a data set and this term is always happening doesn matter what the problem you solving sorry you solving positive negative Now we want to talk about the easiest classification It called linear discriminative"
video_001,960-1020,"analysis. Is this is another corrective simplest, simplest classification or LDA. So LDA is one of those classification techniques, I think, that's really came to the game as a way back in the days, the whole university that the whole idea behind this really strange forward, but it still is a really decent classifier. You know, is the silk maybe out here from so many of the classification that they are out there. Now, linear discriminative analysis. So first, what type of classification would you say it is? Generative. Don't, don't call for that term. That's a good question someone asked. So this is generative classification. Now, I don't know why they put the term discriminative in it, but so. So it's generated that means in order to calculate the probability of y being k given x, you need to have"
video_001,1200-1260,That's the first assumption that they made in the LDA paper.
video_001,1260-1320,"Okay. And again, they made the assumption, but they proved it also analytically that it proved this works. Now, what do you think is the dimension of mu? Right? So if you have multivariate Gaussian, you have D columns, you will have one mean per column, right? So this is, is D by one. What's the covariance matrix? D by D, right? Now, and this basically is... That's a multivariate, Gersh."
video_001,1320-1380,"Okay, Gersh. Sorry. Sorry. X1 and X. So we are saying that X1 and X2, they are coming from a multivariate Gaussian. What was the properties of multivariate Gaussian? One was that, so each of these that I mentioned individually, they are a univariate Gaussian, right? They have some mean, which you have the mean, and they have a variance, right? And together, if you project this points, if you can"
video_001,1380-1440,"multivariate Gaussian like this, which that becomes the covariance, right? So also remember the covariance, for example, the two by two is a a a so now Now, that's just for whole data. In the problem that now we are solving in classification there is a little change over there And is that because you have two class, right? You typically have two clusters of data. One is that, for example, this is class zero and this is class one. So, and if you think about it, what basically this means that if you have this data set"
video_001,1440-1500,"and this data set is a table versus a chair images, what you need to do is that first split the data to separate parts. Now you have data of all chairs, data of all tables. And then if you fit a Gaussian, multi-variate Gaussian pair class, right, you will have one like that and you will have only like that. You remember we had in the first class we had this mixture of Gaussians, right? If you have two Gaussians like this together, this is one Gaussian and the larger Gaussian, this one was basically some weighted sum of each of each individual basically Gaussian, right? That becomes mixture Gaussian. So in the scenarios like this, this, your data, so this is multivariate Gaussian, but this technically is a mixture of Gaussian what it calls, because we have this classification, you have different Gaussian data distribution,"
video_001,1500-1560,"and together is a mixture of Gaussian. Now, you're interested, basically find that line again, that separates this from each other, right? It was x equal to 0. Now, let's talk about that line decision. Fine. Decision boundary. If I ask you, hey, how would you describe if I just give you this line and I'm asking that, how would you in a lame language describe that line? What would you say? Don't say the line that separates glasses from each other. No, but what is the definition of this line? But so let me rephrase my question."
video_001,1560-1620,"How would you if I asked you that how would you define this No it doesn have mean series a a line right Okay what is the definition of a normal any line 22 points. So it should be constant. Not two. Any points which are present so that it can get a set of points that you connect them together, right? That's the line, right? So, this decision, the decision line, decision boundary, is a line. That means these are technically set of points. Otherwise, line doesn't have any meaning. Set of points are connected with it. So the definition of a decision line or decision boundary in classification is set of points that the probability of belonging to both class are the same."
video_001,1620-1680,"That's the decision boundary. points either could be class zero or class one. The 0.5 the probability of belonging to both class is the same. So set of points basically, x and y set of points that probability of x, probability of y being class one given x is the same as probability of y being class zero. even x that becomes decision line and that's holds for all sort of classification if you want to solve always that line that means that any point that on that line doesn't cannot belong to any of any class you know and remember this should be the probabilities are the same you know so because this probability technically means 0.5 right it should be 0.5 0.5 this completion should close one if you say probability is zero then one becomes you do the other one"
video_001,1680-1740,"becomes one it should be exactly zero point five so all three points nine count is zero point exactly because if it's zero point four nine and zero point five one you choose this one it should be exactly zero point five to be equal and it cannot be zero because probably on one class zero the the other class is zero, that's wrong. Should be one minus the other one becomes basically neutral. It's not coming exactly. Yes, that's it. So probably of both class is the same. But that's the decision boundary. Now what we want to do is that we want to use this definition to find that decision boundary for LDA problem because we say that any classification doesn matter generative discriminative your goal is to find a line right so we know that the definition of a decision boundary basically means that the"
video_001,1740-1800,"probability of this term should be the probability of this, right? That's the term you want to solve. Now in LDA, we know what's the probability of y being one given x that comes from this times this, a likelihood times the prior, right? So it means that probability of X being one, sorry, X given Y, being one times probability of Y being one, will be the same as probability of Y. X given Y equals zero. Great. Now,"
video_001,1800-1860,"So, you know, this is a multivariate Gaussian that comes from here. So this means one over 2p, you were to, this is one. So that's the first term. For the prior, I just use the same thing. Like a pi of one doesn't matter. And this is equal to"
video_001,1860-1920,Okay. This term is motivated.
video_001,1860-1920,"Gaussian, as I just wrote as a pi one, same multivariate Gaussian times pi zero. So, now, we talked about LDA, we said the first assumption that LDA makes is your data, the likelihood comes from multivariate Gaussian. Now there is a second assumption that LDA makes here."
video_001,1920-1980,"Second assumption basically. And LDA says that covariance of both class are the same. the same. Okay, so what's the meaning of covariance of two data sets are the same? Is that the shape of the distribution is the same? Right? So that doesn't mean that the data are the same. If you have these two data sets, the values are different, but exactly the shape is identical. Okay, so the second assumption that LDA does is that the covariance of these two class are the same. And now why did they make that assumption? And in LDA case is for simply in order to be able to solve this equation. Because if you make an assumption, the covariance are the same, this term, and this term, they cancel each other, right? Because the"
video_001,1980-2040,"covariance, there is no longer variance of one and zero. Those two terms are the same, right? So So, now, okay, first we get rid of this two term. How would you simplify this equation? What would you do? Log, right? We have point log to this. This becomes one over two. I'll say drop the indices here. Log of, equation from type of summation. Does that make sense so far So now what you need to do is basically bring the equation to the other side Actually before bringing the other side I going to have to simplify this term"
video_001,2040-2100,"So in order to simplify this term, what should I do? I should apply the transverse in this equation and do multiplication, right? we have minus one over two x transpose times covariance inverse times x. x transpose covariance inverse x minus one over two x transpose covariance inverse minus mu one plus plus one over two x transpose variance inverse mu one. Then this one times this one it becomes positive times this times that becomes one over two mu one transpose variance inverse x one."
video_001,2100-2160,"and this becomes a negative term because one is one over two. You want to use inverse. You want plus log of p one. I wanted to simplify this term to take it to the other side. The first thing is that, wait, does this make sense? This is what the patient. So all I did apply transpose to this term becomes x transpose minus mu transpose and multiply element wise by this by that. And expand it becomes that one right. So then I have log of p1. I have log of p0 the other side. If I bring it to this side becomes log of p1 minus log of p0. The second is log of p1 over p0 right."
video_001,2160-2220,"So it becomes plus log of five, one, four, zero. And then now this term, I have this term left. So I will do a multiplication and I take it to the other side, basically add a multiplied by another negative one. So first, this becomes transpose becomes a negative, negative, negative, okay, it's a negative, but it goes to the other side, becomes a positive. This is plus one over two x transpose variance inverse x Then this is a negative as a negative because positive and goes to the other side becomes negative minus one over two x transpose variance inverse mu zero. And this is negative negative positive positive"
video_001,2220-2280,"because the other side comes negative minus one over two mu zero transpose variance inverse x. Negative positive negative positive one over two mu zero that finance inverse right and it's equal to equal to zero because we took everything to the other side of the equation. So, now, what we want to also focus first actually we can do this term and this term exactly the same. One is negative, one is positive, right? Because x doesn't have any indices. So the second thing is that if you're looking at these terms, this term and this term are"
video_001,2280-2340,"the same, right? One of them is transpose of the other, right? We have seen similar to this before, so we can just basically use both together. So this becomes, let me write it down here. that becomes x transpose u in this term and this term are the same term minus one over two. You want transpose variance inverse mu one. Plus log. Here also the same. These two terms are the same. This one and this one."
video_001,2340-2400,If you have y transpose on either gives you the other one. Right. So. This becomes minus X transpose zero We have plus one over two Here is zero We should have a transpose. You guys should look at that. Follow what's happening.
video_001,2400-2460,"So, now, does this make sense? I said yes, I said no, why are you such a quiet class? Okay, so, now we need to simplify this. This term and this term, they have x transpose in common. We can say x transpose times variance inverse u1 minus variance inverse u0. Then these two term, I think all they have in common is minus 1 over 2. This was mu transpose minus 1 over 2."
video_001,2460-2520,"and then close that log 1.4 equals 0. What does this remind you of? What could you say about that equation? the side of the dimension. What is the dimension of this multiplication? D by one, right? D by one minus D by one and D by one. So whatever it's coming out of here is D by one. And is this D by one? So also you agree this when you say D by one,"
video_001,2460-2520,"This is a scalar vector, right? We have a multi-pringant covariance by mean,"
video_001,2520-2580,"it gives us some scalar vector, right? So here also, what's the dimension here? So we have one by d, d by d, that's one by d times d by one, one by one, right? So this is a scalar technique, right? So it's one by one. And this is also a scalar. So what does this remind you of? X times a vector and the scalar vector minus some basically constant. A x plus b equals zero. the equation of the line, right? X is the data point, your vector, right? You already"
video_001,2580-2640,"have these terms like your covariance and the mean. You multiply your data point by some scalar vector minus some constant value equal to zero. And that's the decision boundary. And that's how we solved it, right? In order to reach this equation, what did we do? We wrote set of points at the probability of both class one equal, right? We said probability of y being one given x is the same as probability of y being zero given x. Then we said in LDA, you calculate this probability because it's generated as likelihood times prior. like you times prior equal like you times prior for class class one and class zero and we expanded we made a second assumption that the covariances were the same so you get rid of some of the uh basically indices covariance indices and we just expand this term that"
video_001,2640-2700,"multiplication and we took everything to the other side of the line after you simplified all you reach is the equation of a line right that's what you call linear discriminative analysis the decision bond, decision boundary or decision line is simply a linear line. Okay does that make sense So this is LDA and this is like one of the simplest classifiers Now Now there is a second basically discriminative analysis model called quadratic analysis."
video_001,2700-2760,"You, as the term quadratic comes that means that the decision boundary is no longer a straight line. right, so it should be basically some sort of quadratic decision boundary right. So. Now, in QEDA, the first, the first assumption is still there, you know, that means that probably of x given y equal to a comes from the Gaussian with multivariate Gaussian. First assumption is still the same. You assume that your likelihood comes from a multivariate Gaussian. However, we relax the second assumption basically."
video_001,2760-2820,"That means covariance of class one is not the same as covariance of zero. So, now you're basically when you want to solve for this, you need to do the way that you're looking at this. Okay, in order to solve this problem now, I need to do the same thing. I need to solve for the decision boundary by finding this term. Right? And this term is a likelihood times prior previously when you try to simplify it we said the commands are the same so we got rid of one part of the equation but this time if you solve this i will leave this one to yourself you cannot get rid of that the first part you need when you apply log it comes log of one minus log of 2p plus minus log of variance one over two that's the only term is going to be added"
video_001,2820-2880,"and if you simplify it at the end you going to reach to plus bx plus c equal to zero First one, because we get rid of the covariance, we can become a linear line. The second one, you will end up with a basic quadratic term. That's what it becomes qd. You know, so we'll leave this that one to yourself to simplify. Now In both scenarios, we think the first, basically the first step in classification is that you find that decision. Because if you have the decision boundary, you can, with respect to that, you can say if the point is class zero or class one. Right? So assuming that, okay, we have the decision line. Now in the next step is that, okay, for a given point, this is the state, this is a"
video_001,2880-2940,"test data. You want to see what is the chance this test point belongs to a class K, right? Now you have the decision line and for all test point or train point, any point basically, you want to say what's the chances belongs to class K? And this is still the same, right? So we find Y being K times probability of Y being K, right? And in order to solve it, to simplify it, we actually, in the prediction, we apply log to this term. We're basically going to solve log of, so it's going to be log of one minus log of, minus log of,"
video_001,2940-3000,"And that log and indeed cancel each other. And then there is a term log of E k. If I apply log to this term, that's what I get. The first one is a multivariate Gaussian. I apply the log to it. The second one is just a prior, right? So in order to make a prediction for any points, you calculate this value. Now let go back to LDA first So in LDA when we wanted to find the decision line we said okay the covariance are the"
video_001,3000-3060,"same, right? So we got to the equation of the line. for the in the prediction itself, we have a covariance term, you need to have a value for that, right? So in LDA, when you want to solve the prediction, we assume that the covariance is just identity matrix. Right? Just assume the covariance is identity matrix one, basically. If you put one here, so this term goes to zero, basically, right? So this one, basically log of one also goes to zero right where is this one. Inverse of one that's also just one right so this term basically become minus log of this you can come behind this minus minus one over two"
video_001,3060-3120,"plus correct so in lda when we want to make that prediction we first because we ultimately assumption the covenants are the same. So how that means that the covariance shouldn't have any impact in the classification. So we want to get rid of it. The easiest way is that to put the covariance as identity matrix. All the terms will disappear, right? So now let's look at this term. This is a probability of y being in class. anyone tell me that what do you think is the is is this term positive or negative solid for any value"
video_001,3120-3180,That would be good.
video_001,3120-3180,"Now, in, if you look at here, this term, doesn't matter for which class you're solving, it's the same for all classes, right? It's just these the dimensions is the same for all class. So you can basically get rid of that in your calculation, right? It's a constant that exists for any class that you are solving. The only terms that are changing are this term, this term, right? The first one is a negative, is always a negative term."
video_001,3180-3240,"And the second one is a log of probability. What was the shape of the log? This is P. What's log of P? So it's from here to here, right? Zero to one. So log is always negative. Doesn't matter what's the probability. So that term is always a negative itself. Right. So when you are calculating this. Right. It's giving you a negative. Value. Right. So now. Let's try to break it down. See what was the meaning of that. So we have class zero. We have class one. Okay. So in. When we were talking about the decision boundaries, when we were talking about the decision boundary,"
video_001,3240-3300,"I said, our goal is to find this decision boundary, right? But for in the discriminated classification, you simply calculate a value and you say, if it's positive or negative, you can make a decision, right? LDA is a generative classification. I said it's no longer like that. You basically, in order to find the value, you multiply the prior times the likely. Now, let's look at what's the impact here. We have two points, we have two class. Class one, class zero. That's the mean of the two class. Imagine like in 2D scenario. This is one new data point will come here. X test. and you're interested to see which point this belongs to. Based on the probability concept here equation what you need to do is that so what does this term remind you X minus mean transpose terms X minus mean Euclidean distance right So you simply you have the means"
video_001,3300-3360,"for each class and it's d by one, right? These are d by one, this is d by one, your input also is d by one, right? so you do points minus mean right centralize them times dot product so that gives you euclidean distance right so this becomes uh let's say um distance two this stands one and you have two distances but it tells you in order if if we if we just ignore the formula and we have a distance and you want to make a classification, you would have seen the one that has a lower distance which belongs to that. But in the generative, we don't stop there. We have that log of prior."
video_001,3360-3420,"The log of prior. So let's make an example. Let's say you have prior for a cat. is 0.4, or for dog is 0.6. Okay. And this is cat is dog. And you you get one new point, okay. And you calculate this, you can get distance. And you find that let's say that you clear that distance for this side is minus 0.3. And this side is minus zero point. The distance is not minus it has a minus behind it. But that's what I mean. So minus 0.6. Okay, the one of them is further again, the minus comes from this term. Otherwise, this is positive. Now,"
video_001,3420-3480,"is LDO generative approaches that in order to not make a decision, some this term plus log of the prior, right? Can you, using your calculator, tell me what's the log of 0.4 and log of 0.6? Minus 0.39. And the other one? Minus 0 Your point Very true So now you need to sum these two values right So minus one over two times this becomes minus basically becomes this value minus 0.39 and this is minus 0.454, right? And the other one is"
video_001,3480-3540,"minus 0.3 minus 0.22 minus 0.52, right? So you're trying to find the distance, right? Which one of these is closer? The second here is this one, right? So when you want to pick the value, now you are going to pick the dog. Because based on the distance for that one data point, you're closer to this. But generative probably tells you that don't just look at the distance, look at the prior. What is actually the chance being dog based on the data versus cat? So see, the generative probability, first you find the likelihood, then you look at the prior and then you make a decision. In discriminative, we will see we don't have this."
video_001,3540-3600,"only calculate the likelihood which is for example the distance and we make a decision based on that we don't care about a prior concept is that clear okay so now so this was for lda that became aligned now in um when we are working with qda because we don't have the, like we cannot just drop the covariance anymore, so you need to calculate the covariances. So, let's say covariance of plus k is this one over n k summation of I guess that's the if you estimate the covariance for each class there's a"
video_001,3600-3660,"multivariation and then for me basically you will have simply just take average of points A division of Xi or K where I is very simply for mean you average the points that belong to class K that becomes the mean of each class, right? for covariance you need to cut and this is where points belong to class k not for all points. So in Qt you need to imagine this equation is from your book. That's the equation for the covariance and mean of multivariate. There's a bias estimate of the covariance also. Anyway,"
video_001,3660-3720,"so you have this and the prior is simply basically a summation of points over the total data points that you have where i that becomes the prior right you count how many because when we say summation because because it's actually. And then summation was it. If you have it for multi class classification, we just say NK over number of point in class K divided by total points that becomes the prior. So. In QDA, you have these three terms. So if you want to calculate this equation, you cannot just get rid of this term anymore or this term, because now you have a covariance. So you need to actually use that covariance"
video_001,3720-3780,"to make a prediction. So remember that one. The same equation, but then you cannot just drop the covariance matrices. You need to use the value that you have found. Then comes QDA in the prediction step. Now, there is one special case in QDA. And in the special case, rather than using so many covariance, they do an average covariance and in the prediction step and they want to use that. Okay, so the average covariance basically, you use summation and our variance or are from one to K. You simply multiply each covariance by how many points was in that"
video_001,3720-3780,"that class, you sum this and then average this and it gives you one average covariance. And you use that in your classification."
video_001,3780-3840,"So that's a special case of QDA. But remember that you might have questions like a formula given and asked you that, something like that. Question. So this is LDA and QDA. QD and this is like again, the simply one of one of the simplest classifiers, because the idea behind is really straightforward. You just find the decision lines, which is like they got multivariate Gaussian. Once now you get rid of the covariance as you assume you're the same in the other scenario you need to calculate this one of them deadline becomes linear. The other one becomes quadratic. Okay, so. Can you give an example of a data set where we can apply this, the quadratic one? I will not give an example, but you guys need to solve it. I gave this example, I think this was last semester's question."
video_001,3840-3900,"So last semester I'm going to tell you how was the exam. in case in one practice. I give you one data set in your in your exam, 10 data points, 12 data points, 20 data points, and I give random questions. One of them is linear regression, one of them is LDA, one is QDA, one is logistic regression, and randomly each of you get one of those. So that's the whole idea. So a lot of the students are like, I didn't practice this one, I didn't understand that's on you. You need to know all those. We are going to learn for me times six algorithm, linear, QDA, LDA, they're just irrigation, naive-based, and there's another classification. So you need to basically be able to, even in one data set, solve it manually. That's all I'm telling you. That's your question is me to be examined like that. Don't come to an exam and say that I read linear irrigation, I forgot, naive-based. No, I'm already telling you that you need to know all six."
video_001,3900-3960,"And you need to be able to solve it. start practicing. If you have a question, then I'm not going to give a question to go solve. You can internet is full of data set. You can generate a random data set and just simply manually solve. Okay Now we want to go back now to the problem that we were talking about So we so we were talking about that okay in general form of classification because we care about the probability of, so right now, this is what we saw, we were predicting for one day, come X test, right, and see that we calculate that here. But in general form, we say if this is the. This is a decision line."
video_001,3960-4020,"Right. When I say general from the right time we're talking about discriminating. In discriminative form, you have decision line and you care about you either this side of the line or you are the other side. Right. So if you are this side of the line, this term is positive. if you're at this side of the line, this term is negative. And if you're on the line is zero. Right. And if we want to formulate that, basically, we will say that if this value is greater than zero, y is one. If this value is greater than less than zero, y is zero. And so because of this, if someone asked me that, OK, hey, what is the probability of this data set, even this input, right, because there are again, IID, this becomes multiplication of individual."
video_001,4020-4080,"And why are you doing x i? And because we are saying that we care about classes one, that means you're solving this with respect to class one, and we know that if If point belongs to the other class, right? The quality of class one goes to zero. So that equation, that multiplication becomes zero. So that's a problem. That means that when we want to make a classification, we shouldn't be radius three. We shouldn't say that if you are this side of the line, you are this class, you are the other side of the line, you are the other class, right? We shouldn't be that sure about our prediction. Now, the solution that we are starting to apply here was that, okay, you know, we have these points, right? For both class. But how would you guarantee that if the point is here actually belongs to class one? This is class one, this is class zero."
video_001,4080-4140,"How would you guarantee this point belongs to class one Is the other side of the right line correct But how sure are you Right So here we are answering it by just being really sure I'm sure if it's positive, it's plus one. And we saw that cause a problem. Right. That can cause a problem. So what would what should we do? What we are going to do is that we are going to. employ a specific function on the points. So this is the distance theta transpose x, correct? It's either positive from the line or negative under the line. So we are going to apply a function that as the distance increases, the output of that function increase. As the distance decrease, the output of the function decrease. We're going to basically apply a function that is with respect to a distance that means if you're closer to the line you're closer"
video_001,4140-4200,"we can give you a lower value that means i'm not that confident if you're further away from the line doesn't matter this way or this way it gives you a higher value what function would you think that can give you that it's not log think about this all the simple functions that we have in algebra. What function do you think is increasing if the value increases? Let's see what this binary. It's exponential, right? Because basically e, if this is, I don't know, if this is like, let's say p, that's e of p, right? So as it increases, it gives you the larger value, right? So, we know that if we apply E over the distance, as the value of distance increases, or that's"
video_001,4200-4260,"because we are further away from the line, E is going to give us a larger value, more confidence. And as we are closer to the line, as we get closer to the line, the value is going to be smaller. So, first thing we are going to do is that we are going to apply E to this term, to this equation. We're going to say, I'm fine. Right. So this becomes e to the power of theta transpose x. Each of the power of. So. Negative theta transpose x you know for mass simplicity and also put in order to some So if you want to know properly we going to add 1 over 2 theta transpose x This one over two theta transpose x So the first one over two is just because we want to sum of this two e to be the same to complete each other. And negative is because one of them is the negative side. You know, the other one, this one has a negative distance."
video_001,4260-4320,"You multiply it by a negative, right? The other one has a positive distance. It doesn't need that negative term. Now. Okay, so now we are saying that if, for this term, for class one, but this term for class zero, but we say that we are going to use this E to give us some confidence. Confident is some probability technique, right? But this term is not a probability. How would you convert these two terms to a probability? If you have two terms, how would you make it a probability? Okay. divided by total, right? So all you need to do is that basically, let's say we define one over z, e theta transpose x, and one over z, e minus theta transpose x. And it defines z to be basically summation of these two."
video_001,4320-4380,"Now, now you are sure this is a probability. That means that every time you pass to this equation right now, a value, and see that, hey, which class this belongs to, it doesn't give you one or zero, it gives you a probability. I'm 60 percent sure this class one, and class zero becomes one minus 60 percent is 40 percent right so the the model you basically this type of model the allow you to make a decision okay if well we're going to get back to you let me first write this and i'm going to see what the decision means property of y being class one given x that's equal uh one over z Z and probability of Y being class zero given X."
video_001,4380-4440,"So, when I say your model, the model this way allows you to make a decision. Think about this way. So you're making a prediction,"
video_001,4380-4440,"And you have this model that this model is for each point is going to give you what's the probability this point belongs to class one, not class zero, because we always predict with respect to class one. So. Like I have this table. And this is coins. This I have one dimension X and this is Y hat what I'm reading for today. and should just be rephrased the Y hat to be probability of Y being class one given X."
video_001,4440-4500,"But we have all these data points. It has to do this model, this equation. I mean, this is assuming that they already have found that theta. OK, that we say that we haven't gotten to that point. Let's assume you have the data, you have this model right now. For each data point, you pass it to the model, it gives you a value. One, 0.2, 0.001, and so on. It gives you value. If the probability, higher the probability, that means you're actually below the class 1, the probability means you don't below the class 1. So the benefit of such a model is that, You know, by default, how would you decide which point should belong to class one actually, if you are having these values? How would you decide that threshold? Maybe go 0.5. That's like a really intuitive scenario, right?"
video_001,4500-4560,"0.5. So you decide anything, you find a cutoff anytime it's above one, you say 1, 1, 0, 0, 1, 0. right? That's kind of like a really basically intuitive approach, like a naive approach. When you have these scenarios that you have these probabilities, something that we're going to learn is called sensitivity analysis. Sensitivity analysis means that you are looking at the values that your model predicting. Okay, So let's say I have this model and my model, if this is the probability value, or actually if this is like this is the probability, right? So if this is P this is the frequency frequency right So probably start here let say from zero let say I created"
video_001,4560-4620,"these beans, right? 0.2 all the way 0.8, 0.9 and one I created the speeds by the incrementing by 0.1 or 10. basically. And I make a prediction over all my training data. So I have 1 million training data after I finished learning, I pass all 1 million to the model and says that what is the probability of going to plus one for each. And I create this frequency count. Now I have a lot here. I have some here. have some here increasing which is 0.5 maybe i'll have a couple of here and then i have both here i have not here this is technically what it's going to look like now when you are looking at something like this first thing is that you always want your model to not to be too"
video_001,4620-4680,"confident what does that mean the confident model is that the model that always really sharply predicts around zero or predicts around one. You don't want the model to be that confident. You want the model to have a distribution. You know, that gives you value 0.6, 0.7, you know, something like that. Overconfident model, it causes problem in prediction. In the reality, when you're sending it outside, when your model is overconfident, you always either says 90% and like class one or zero percent class two. doesn't give you basically an option to make a decision. And what's the decision? So we are looking at this line. And then you also let's say it's like you're doing some visualization, you also call the read for class one versus class zero. And you have these colors, right? You will see here, for example, this"
video_001,4680-4740,"part, or cloud, those points belongs to class zero, and you made a mistake, right? And there is this class one. And you will see, actually, let's just raise it. There's no class one here, but you will see there are some class one points here, and there are some class one here, and there are some class zero, because the dark is class zero. And this empty one is class one, so there's no class zero in here. Now by default when you want to define that threshold you would have say 0 Anything this side you will say class one and the other side you say class But depending on the problem that you have so an example for you is that in my field of work, we do cybersecurity, like we deal with threats, right? So it's like, and in a bigger system, like a hospital environment that attack happens, we need to be really precise on stopping before the whole network goes. So we have these models that, you know,"
video_001,4740-4800,"imagine the two-minute threat detection and we need to mitigate this cause to mitigate, to stop the threat. That means to block everything, to isolate the whole network. But think about it. If we make a mistake, we think it was an attack and it's not actually an attack, you shut down the whole system and that's really cost, you know, it's a hospital, It's not as cool that you can shut it down, right? So we need to be really sure about when to cut, when to stop. So we do something like this sensitivity analysis, then we are looking at the actual data and we see that there is a chance I make a mistake here. There's a chance I make a mistake here. I don't make a mistake here. So I would actually go pick up my cutoff value there. see, do you look this whole sensitive analysis, you look at the prediction of your models, then where the where the model makes more mistakes. And depending on what is more important for you,"
video_001,4800-4860,"you know. So in my case, in our case, we care about to detect correctly, but we also care to not call user fatigue. Now, if you keep making alerts, and the user is like a, have you heard that the story that the shepherd guy kept calling wolf. So if you keep doing that, at some point, when you see wolf, the user doesn't care. That's called user fatigue in software. So you don't want to keep an alert to your user, and then they got tired of it. So you try to be really sneaky. So okay, then that question is that, so what would you do to this point? You know, there might be actually an attack or something. Yeah, so you still do something about it. You don't stopping you directly called basically there's a system like a messaging system you contacted the hospital says hey something like this i'm not 100 sure on it but it's dangerous take a look so you even let the it know so they go basically look into it so that's why this concept of probability"
video_001,4860-4920,"is actually important for you now by converting from this system to a problem this is you not only you are saying that you know here we were saying surely yes or no but now you have you first you are saying the probability but also that poverty gives you an option to do sensitivity analysis okay so this concept is really important remember that now okay here. So I'm going to just write this term with respect to class one. Now, we have class one given x, we say this was equal one over z, the power of one over two, is that's what's x. So then"
video_001,4920-4980,"z is basically this one plus the other one. So this is one over two theta transpose X over theta transpose X plus e to the power minus theta transpose X, correct? So now simplify this. So this becomes Okay, extracted that term from. Okay, now I can cancel this too. Who knows what's that function? What's the function?"
video_001,4980-5040,"The sigmoid function, right? It's a sigmoid function. And now what do you think is this classification? Logistic regression. The way that we basically develop logistic regression was to formulate the simplest scenario for classification. The decision line, if you're up or down, you're class one or class zero. And to just avoid being so sure, we applied an e-tweet by just simply applying an E function and normalizing it to become a probability, that becomes a logistic regression. Make sense? This is also, you will see it this way. Okay, data x, sigmoid of a black."
video_001,4980-5040,"is one over one plus into the power. Remember that this type of equations, these are something that you actually need to memorize it for your whenever as data scientists,"
video_001,5040-5100,"data analysts, machine learning, you need to function like sigmoid functions, softmax, you need to know the formula by heart, you know, this is a straightforward. and we can figure it out. Okay, so this becomes a sigmoid function, and this becomes a logistic regression. Now, we haven't solved for logistic regression yet, we just formulate, we just formulate, okay, the probability of, let me actually write behind, the probability of y being class one given x x equal to sigmoid function. What do you think the probability of YB class zero is one minus that one minus this term is typically this becomes a positive that becomes a volume plus zero. So we will continue on this one next class. But what we want to do in next class is basically now you have the probability, right?"
video_001,5100-5160,"You remember we said this term the probability of Y being one X one from one and previously this multiplication would have gone to zero if you have one mistake right But now you have this same way term, right? So also, let me point out something here. here. So when I say we care about class, that's one, I from one and even if you write this just with respect to a x and k is y. Even if you don't write it only with respect to class one, we write it to both class. Look at this multiplication for a given point."
video_001,5160-5220,"Suppose your points, right? X and Y, one, one, zero, zero, one, zero, something. This is the first one, the point belongs to class one. You would have said, what's the probability that's class one? And let's say your model says correctly one. That's one times, for the next one, also your model says correctly one times one. For this one also, you're asking the model, what's the probability of being class 0? The model correctly says 1, right? So it's a still 1. But if you make one mistake that means that you one point you belong to class 1 you say it class zero or it class zero you say class one this term goes to zero See it doesn matter how you write it, if you just write the class one or with respect to total, one mistake is overall goes to zero. But now, the way to use this term is because it's probability, so it doesn't basically push it towards zero, right? And you can see, you can guess how we're going to solve it. By log, I assume like estimation point the theta that's the next step is here okay"
video_001,5220-5280,stop here we're going to have Now. What is great? Thank you or
video_001,5280-5340,yeah Thank you. Thank you.
